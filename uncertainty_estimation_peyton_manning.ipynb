{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ourownstory/neural_prophet/blob/master/example_notebooks/uncertainty_estimation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Uncertainty Estimation\n",
    "NeuralProphet allows the modelling of overall prediction intervals. We use the standard pinball loss function in the model to estimate different quantiles specified by the user, without any other distributional assumptions. Here, we demonstrate this feature using the peyton manning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                 1.11.0\n",
      "torch-lr-finder       0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if 'google.colab' in str(get_ipython()):\n",
    "#     !pip install git+https://github.com/ourownstory/neural_prophet.git # may take a while\n",
    "#     #!pip install neuralprophet # much faster, but may not have the latest upgrades/bugfixes\n",
    "#     data_location = \"https://raw.githubusercontent.com/ourownstory/neural_prophet/master/\"\n",
    "# else:\n",
    "#     data_location = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-12-10</td>\n",
       "      <td>9.590761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-12-11</td>\n",
       "      <td>8.519590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-12-12</td>\n",
       "      <td>8.183677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds         y\n",
       "0  2007-12-10  9.590761\n",
       "1  2007-12-11  8.519590\n",
       "2  2007-12-12  8.183677"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from neuralprophet import NeuralProphet, set_log_level\n",
    "# set_log_level(\"ERROR\")\n",
    "df = pd.read_csv(data_location + \"example_data/wp_log_peyton_manning.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functionality\n",
    "For estimating the 0.25 and 0.75 quantiles, the NeuralProphet object can be created as follows by\n",
    "specifying the required quantiles (in between (0,1)) in a list. The 0.5 quantile is always modelled by default.\n",
    "The progression of the PinballLoss values through the training epochs can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 98.967% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4059eb218ce64e359a4e242a04462328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb#ch0000009?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m NeuralProphet(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb#ch0000009?line=1'>2</a>\u001b[0m     quantiles\u001b[39m=\u001b[39m[\u001b[39m0.9\u001b[39m, \u001b[39m0.1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb#ch0000009?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb#ch0000009?line=3'>4</a>\u001b[0m metrics \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mfit(df, freq\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevin/workspace/neuralprophet/neural_prophet_kevin/uncertainty_estimation_peyton_manning.ipynb#ch0000009?line=4'>5</a>\u001b[0m metrics\u001b[39m.\u001b[39mhead(\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/forecaster.py:661\u001b[0m, in \u001b[0;36mNeuralProphet.fit\u001b[0;34m(self, df, freq, validation_df, progress, minimal)\u001b[0m\n\u001b[1;32m    659\u001b[0m         metrics_df \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m         metrics_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(df, progress\u001b[39m=\u001b[39;49mprogress)\n\u001b[1;32m    662\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     df_val, _, _, _ \u001b[39m=\u001b[39m df_utils\u001b[39m.\u001b[39mprep_or_copy_df(validation_df)\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/forecaster.py:2140\u001b[0m, in \u001b[0;36mNeuralProphet._train\u001b[0;34m(self, df, df_val, progress)\u001b[0m\n\u001b[1;32m   2137\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_minimal(df, progress_bar\u001b[39m=\u001b[39mprogress_bar)\n\u001b[1;32m   2139\u001b[0m \u001b[39m# set up data loader\u001b[39;00m\n\u001b[0;32m-> 2140\u001b[0m loader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_train_loader(df)\n\u001b[1;32m   2141\u001b[0m \u001b[39m# set up Metrics\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhighlight_forecast_step_n \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/forecaster.py:1923\u001b[0m, in \u001b[0;36mNeuralProphet._init_train_loader\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_model()  \u001b[39m# needs to be called after set_auto_seasonalities\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_train\u001b[39m.\u001b[39mlearning_rate \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_train\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_train\u001b[39m.\u001b[39;49mfind_learning_rate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, dataset)\n\u001b[1;32m   1924\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mlr-range-test selected learning rate: \u001b[39m\u001b[39m{:.2E}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_train\u001b[39m.\u001b[39mlearning_rate))\n\u001b[1;32m   1925\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_train\u001b[39m.\u001b[39mget_optimizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/configure.py:210\u001b[0m, in \u001b[0;36mTrain.find_learning_rate\u001b[0;34m(self, model, dataset, repeat)\u001b[0m\n\u001b[1;32m    208\u001b[0m lrs \u001b[39m=\u001b[39m [\u001b[39m0.1\u001b[39m]\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n\u001b[0;32m--> 210\u001b[0m     lr \u001b[39m=\u001b[39m utils_torch\u001b[39m.\u001b[39;49mlr_range_test(\n\u001b[1;32m    211\u001b[0m         model,\n\u001b[1;32m    212\u001b[0m         dataset,\n\u001b[1;32m    213\u001b[0m         loss_func\u001b[39m=\u001b[39;49mloss_func,\n\u001b[1;32m    214\u001b[0m         optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    215\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     lrs\u001b[39m.\u001b[39mappend(lr)\n\u001b[1;32m    218\u001b[0m lrs_log10_mean \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([np\u001b[39m.\u001b[39mlog10(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m lrs]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lrs)\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/utils_torch.py:46\u001b[0m, in \u001b[0;36mlr_range_test\u001b[0;34m(model, dataset, loss_func, optimizer, batch_size, num_iter, skip_start, skip_end, start_lr, end_lr, plot)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mHiddenPrints():\n\u001b[1;32m     45\u001b[0m     lr_finder \u001b[39m=\u001b[39m LRFinder(model, lrtest_optimizer, loss_func)\n\u001b[0;32m---> 46\u001b[0m     lr_finder\u001b[39m.\u001b[39;49mrange_test(\n\u001b[1;32m     47\u001b[0m         lrtest_loader,\n\u001b[1;32m     48\u001b[0m         val_loader\u001b[39m=\u001b[39;49mlrtest_loader_val,\n\u001b[1;32m     49\u001b[0m         end_lr\u001b[39m=\u001b[39;49mend_lr,\n\u001b[1;32m     50\u001b[0m         num_iter\u001b[39m=\u001b[39;49mnum_iter,\n\u001b[1;32m     51\u001b[0m         smooth_f\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,  \u001b[39m# re-consider if lr-rate varies a lot\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m         diverge_th\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m         step_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mexp\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     lrs \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m     losses \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py:317\u001b[0m, in \u001b[0;36mLRFinder.range_test\u001b[0;34m(self, train_loader, val_loader, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`val_loader` has unsupported type: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected types are `torch.utils.data.DataLoader`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor child of `ValDataLoaderIter`.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(val_loader))\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    315\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_iter)):\n\u001b[1;32m    316\u001b[0m     \u001b[39m# Train on batch and retrieve loss\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(\n\u001b[1;32m    318\u001b[0m         train_iter,\n\u001b[1;32m    319\u001b[0m         accumulation_steps,\n\u001b[1;32m    320\u001b[0m         non_blocking_transfer\u001b[39m=\u001b[39;49mnon_blocking_transfer,\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m val_loader:\n\u001b[1;32m    323\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate(\n\u001b[1;32m    324\u001b[0m             val_iter, non_blocking_transfer\u001b[39m=\u001b[39mnon_blocking_transfer\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py:377\u001b[0m, in \u001b[0;36mLRFinder._train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    372\u001b[0m inputs, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_to_device(\n\u001b[1;32m    373\u001b[0m     inputs, labels, non_blocking\u001b[39m=\u001b[39mnon_blocking_transfer\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    376\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(inputs)\n\u001b[1;32m    378\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m    380\u001b[0m \u001b[39m# Loss should be averaged in each step\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/time_net.py:689\u001b[0m, in \u001b[0;36mTimeNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultiplicative\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inputs[\u001b[39m\"\u001b[39m\u001b[39mregressors\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    685\u001b[0m         multiplicative_components \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscalar_features_effects(\n\u001b[1;32m    686\u001b[0m             inputs[\u001b[39m\"\u001b[39m\u001b[39mregressors\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmultiplicative\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregressor_params[\u001b[39m\"\u001b[39m\u001b[39mmultiplicative\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    687\u001b[0m         )\n\u001b[0;32m--> 689\u001b[0m trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrend(t\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    691\u001b[0m \u001b[39m# ???\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39m# 0 is the median quantile index\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39m# all multiplicative components are multiplied by the median quantile trend\u001b[39;00m\n\u001b[1;32m    694\u001b[0m out \u001b[39m=\u001b[39m (\n\u001b[1;32m    695\u001b[0m     trend \u001b[39m+\u001b[39m additive_components \u001b[39m+\u001b[39m trend\u001b[39m.\u001b[39mdetach()[:, \u001b[39m0\u001b[39m, :]\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m multiplicative_components\n\u001b[1;32m    696\u001b[0m )  \u001b[39m# dimensions - [batch, no_quantiles, n_forecasts]\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/time_net.py:479\u001b[0m, in \u001b[0;36mTimeNet.trend\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    476\u001b[0m     trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrend_k0 \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(t, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    477\u001b[0m     \u001b[39m# trend = self.trend_k0 * t\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_piecewise_linear_trend(t)\n\u001b[1;32m    480\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m+\u001b[39m trend\n",
      "File \u001b[0;32m~/workspace/neuralprophet/neural_prophet_kevin/neuralprophet/time_net.py:455\u001b[0m, in \u001b[0;36mTimeNet._piecewise_linear_trend\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    452\u001b[0m     m_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39munsqueeze(current_segment, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrend_m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    453\u001b[0m     \u001b[39m# m_t = torch.sum(current_segment * torch.unsqueeze(self.trend_m, dim=0), dim=2)\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrend_k0 \u001b[39m+\u001b[39;49m k_t) \u001b[39m*\u001b[39;49m t \u001b[39m+\u001b[39m m_t\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "m = NeuralProphet(\n",
    "    quantiles=[0.9, 0.1]\n",
    ")\n",
    "metrics = m.fit(df, freq=\"D\")\n",
    "metrics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(df, periods=30, n_historic_predictions=100)\n",
    "forecast = m.predict(df=future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasts df now has columns designated for the different quantiles of the output yhat1. For the\n",
    "components, only the 50% quantile is shown in the current state of this feature. The uncertainty bands can\n",
    "be plotted as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_forec = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the components and parameters can be plotted for the 0.5th quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_comp = m.plot_components(forecast)\n",
    "fig_param = m.plot_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other features of the model, such as events, regressors, regularization etc. can be used along with\n",
    "the quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "playoffs = pd.DataFrame({\n",
    "    'event': 'playoff',\n",
    "    'ds': pd.to_datetime([\n",
    "        '2008-01-13', '2009-01-03', '2010-01-16',\n",
    "        '2010-01-24', '2010-02-07', '2011-01-08',\n",
    "        '2013-01-12', '2014-01-12', '2014-01-19',\n",
    "        '2014-02-02', '2015-01-11', '2016-01-17',\n",
    "        '2016-01-24', '2016-02-07',\n",
    "    ]),\n",
    "})\n",
    "\n",
    "superbowls = pd.DataFrame({\n",
    "    'event': 'superbowl',\n",
    "    'ds': pd.to_datetime([\n",
    "        '2010-02-07', '2012-02-05', '2014-02-02',\n",
    "        '2016-02-07',\n",
    "    ]),\n",
    "})\n",
    "\n",
    "events_df = pd.concat((playoffs, superbowls))\n",
    "\n",
    "# NeuralProphet Object\n",
    "m = NeuralProphet(\n",
    "    quantiles=[0.9, 0.1],\n",
    "    seasonality_reg=1.5,\n",
    ")\n",
    "\n",
    "# set the model to expect these events\n",
    "m = m.add_events([\"playoff\", \"superbowl\"], regularization=0.1)\n",
    "\n",
    "# create the data df with events\n",
    "history_df = m.create_df_with_events(df, events_df)\n",
    "metrics = m.fit(history_df, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(history_df, events_df, periods=30, n_historic_predictions=100)\n",
    "forecast = m.predict(df=future)\n",
    "fig_forec = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Base Loss Functions\n",
    "In the standard pinball loss, the base error is just the difference between the actuals and the\n",
    "predicted. However, in NeuralProphet we provide the user, the capability to define other losses for the\n",
    "base loss such as `MSE`, `MAE` and `Huber`. This can be done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = NeuralProphet(\n",
    "    loss_func=\"Huber\",\n",
    "    quantiles=[0.9, 0.1]\n",
    ")\n",
    "metrics = m.fit(df, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(df, periods=30, n_historic_predictions=100)\n",
    "forecast = m.predict(df=future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Auto-Regression\n",
    "Overall uncertainty can be modelled with the AR-Net as well. The only difference is that,\n",
    "to plot the forecasts, always a forecast step needs to be specified using the `highlight_nth_step_ahead_of_each_forecast` function. The uncertainty bands are plotted\n",
    "for that particular forecast step, since otherwise the plot can look quite cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = NeuralProphet(\n",
    "    n_lags=5,\n",
    "    n_forecasts=3,\n",
    "    quantiles=[0.9, 0.1]\n",
    ")\n",
    "metrics = m.fit(df, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(df, periods=3, n_historic_predictions=10)\n",
    "forecast = m.predict(df=future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m.highlight_nth_step_ahead_of_each_forecast(2)\n",
    "fig_last_forec = m.plot_last_forecast(forecast, include_previous_forecasts=3)\n",
    "fig_forec = m.plot(forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9896633dd2687027a97d37c5dc67af73c090796eacc50847f77e025856fff9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
